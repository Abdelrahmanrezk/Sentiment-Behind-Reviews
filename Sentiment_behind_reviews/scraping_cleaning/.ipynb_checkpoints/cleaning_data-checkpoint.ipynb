{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning scraped reviews\n",
    "\n",
    "**Welcome to second stage of cleaning scraped reviews, what in this notebook:**\n",
    "- Get all reviews from mongo database\n",
    "- pipline process\n",
    "    - one for Arabic reviews\n",
    "    \n",
    "**Pipline process Structure**\n",
    "- Convert all reviews to lower case # for English Reviews\n",
    "- remove punctuations of all reviews\n",
    "- remove stop words # I found its so bad to remove stop words because its convert the meaning of the sentense at all but I have Designed my own stopwords for this project because of sentment analysis issues\n",
    "- spell correction\n",
    "- Tokenization\n",
    "- Steaming\n",
    "- Lemmatization\n",
    "- Frequency of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from souq_configs import *\n",
    "import stanza as sta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import os\n",
    "import pymongo\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import logging\n",
    "import logging.config\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, webtext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import Word, TextBlob\n",
    "from autocorrect import spell\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import unidecode\n",
    "import string\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from nlppreprocess import NLP\n",
    "from tashaphyne.stemming import ArabicLightStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to mongo cloud database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(f\"mongodb+srv://{mongo_user}:{mongo_pass}@{mongo_url}\")\n",
    "db = client.SBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all prodcuts in our cloud database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = list(db.products.find({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Our scraped now are \" + str(len(products)) + \" Product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"one product Data\\n\\n\", products[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Reviews\n",
    "Our data now have all of products info not just reviews, so we need to get these reviews beside of this we need to separate Arabic and English reviews.\n",
    "\n",
    "returned products are list each of them is document(objects as key value)\n",
    "\n",
    "### Function structure\n",
    "- First loop over all products\n",
    "- for each product get all reviews\n",
    "- for each review check its language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all_reviews(products):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of products each of them as object with key and value\n",
    "    return:\n",
    "        from this products we just need the reviews so\n",
    "        Arabic Reviews\n",
    "        English Reviews\n",
    "    '''\n",
    "    all_arabic_reviews = []\n",
    "    all_english_reviews = []\n",
    "# Loop over products\n",
    "    for indx,pro_val in enumerate(products):\n",
    "# Loop over reviews\n",
    "        for review in pro_val['product_reviews']:\n",
    "            try:\n",
    "                char_check = review[0]\n",
    "                if char_check >= 'a' and char_check <= 'z' or char_check >= 'A' and char_check <= 'Z':\n",
    "                    all_english_reviews.append(review)\n",
    "                else:\n",
    "                    all_arabic_reviews.append(review)\n",
    "            except Exception as e:\n",
    "# send exception to log folder\n",
    "                logf = open(\"../logs_files/cleaning_reviews_error.log\", \"a+\")\n",
    "                logf.write(\"This error related to function export_all_reviews of cleaning_data file\\n\" \n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return all_arabic_reviews, all_english_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare my own stopwords list\n",
    "I have designed these stopwords from multiple resource based on this problem of sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_of_stop_words_to_list(file_dir):\n",
    "    '''\n",
    "    Argument:\n",
    "        file with stop words\n",
    "    return:\n",
    "        list of these stop words\n",
    "    '''\n",
    "    stop_words_designed = []\n",
    "    with open(file_dir, 'r') as file:\n",
    "        file = file.readlines()\n",
    "        file = \"\".join(file)\n",
    "        file = re.sub('[\\[\\]\\'\\\",]', '', file)\n",
    "        stop_words_designed = file.split()\n",
    "    return stop_words_designed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_dir = !pwd\n",
    "# file_dir = file_dir[0]\n",
    "# print(file_dir)\n",
    "file_dir1 =  '../stop_words/nltk_stop_words_handle.txt'\n",
    "file_di2 = '../stop_words/stop_list1.txt'\n",
    "file_di3 ='../stop_words/updated_stop_words.txt'\n",
    "stop_words_designed = convert_file_of_stop_words_to_list(file_dir1)\n",
    "print(\"First File\\n\",len(stop_words_designed))\n",
    "stop_words_designed.extend(convert_file_of_stop_words_to_list(file_di2))\n",
    "print(\"\\nAfter Expanding two file together\\n\",len(stop_words_designed))\n",
    "# now remove dublicated via set\n",
    "stop_words_designed = set(stop_words_designed)\n",
    "stop_words_designed = list(stop_words_designed)\n",
    "print(\"\\nRemove Dublicated words\\n\",len(stop_words_designed))\n",
    "arabic_stop_words_designed = convert_file_of_stop_words_to_list(file_di3)\n",
    "print(\"\\nUpdate the end file manually to remove some of words\\n\",len(arabic_stop_words_designed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arabic_reviews, all_english_reviews = export_all_reviews(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Arabic reviews\\n\", len(all_arabic_reviews))\n",
    "print(\"\\nTotal English reviews\\n\", len(all_english_reviews))\n",
    "print(\"\\nSome of  Arabic reviews\\n\", all_arabic_reviews[:5])\n",
    "print(\"\\nsome of English reviews\\n\", all_english_reviews[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove dublicate entry first\n",
    "use set features of python which return all of unique reviews\n",
    "then return as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arabic_reviews = set(all_arabic_reviews)\n",
    "all_english_reviews = set(all_english_reviews)\n",
    "all_arabic_reviews = list(all_arabic_reviews)\n",
    "all_english_reviews = list(all_english_reviews)\n",
    "print(\"Now Total Arabic reviews\\n\", len(all_arabic_reviews))\n",
    "print(\"\\nNow Total English reviews\\n\", len(all_english_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep only reviews more than one words because maybe work like قليلة has no meaning but  جودة قليلة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_those_reviews_greater_1_word(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of reviews\n",
    "    return:\n",
    "        list of reviews each of them grater than or equal 2 words\n",
    "    '''\n",
    "    updated_reviews = []\n",
    "    for review in text_list:\n",
    "        if len(review.split()) >= 2:\n",
    "            updated_reviews.append(review)\n",
    "            \n",
    "    return updated_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arabic_reviews = keep_those_reviews_greater_1_word(all_arabic_reviews)\n",
    "all_english_reviews = keep_those_reviews_greater_1_word(all_english_reviews)\n",
    "print(\"Now Total Arabic reviews\\n\", len(all_arabic_reviews))\n",
    "print(\"\\nNow Total English reviews\\n\", len(all_english_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_of_strings_to_list_of_words(text):\n",
    "    '''\n",
    "    A function used to convert list of strings to be list of all words of these strings like:\n",
    "    ['تليفون جيد',\n",
    " 'ايباد ميني فور من افضل الايبادات في السوق وسعره كان مغري',]\n",
    "    return:\n",
    "        list of all words in all string like:\n",
    "        ['تليفون', 'جيد'] and so on\n",
    "    '''\n",
    "    word_list = []\n",
    "    for i in text:\n",
    "        i = i.split()\n",
    "        word_list.extend(i)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_lower_conversation(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        text as string of words\n",
    "    return:\n",
    "        lower of this string\n",
    "    '''\n",
    "    return sentence.lower()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_lower_conversation(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings and each of these strings does contain some of words\n",
    "    return:\n",
    "        lower each string in this list\n",
    "    '''\n",
    "    text_list = [one_string_lower_conversation(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove some of special characters characters**\n",
    "\n",
    "like: َ\"  ُ  ْ  \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_remove_diacritics(sentence):\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    sentence = re.sub(noise, '', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_remove_diacritics(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of string without special chars from Arabic language\n",
    "    '''\n",
    "    text_list = [one_string_remove_diacritics(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove some of puncatution and repeated words**\n",
    "\n",
    "like: $%&'()*+,-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_remove_punctuation(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    reutrn:\n",
    "        string without punctuation like [.!?] and others\n",
    "    '''\n",
    "    sentence = sentence.split(' ')\n",
    "    strs = ''\n",
    "    punctuations = string.punctuation\n",
    "    for word in sentence:\n",
    "#         word = re.sub(r'(.)\\1+', r'\\1', word) # remove repated chars\n",
    "        word = re.sub('[^\\w\\s+]',' ',word)\n",
    "        if len(word) > 1 and not (word[0] >= 'a' and word[0] < 'z' or word[0] >= 'A' and word[0] < 'Z'):\n",
    "            strs += word + ' '\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    strs.translate(translator)\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_strings_remove_punctuation(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings \n",
    "    reutrn:\n",
    "        list of strings without punctuation like [.!?] and others\n",
    "    '''\n",
    "    text_list = [one_string_remove_punctuation(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_normalize_arabic(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of words but standardize the words\n",
    "    '''\n",
    "    sentence = re.sub(\"[إأآا]\", \"ا\", sentence)\n",
    "    sentence = re.sub(\"ى\", \"ي\", sentence)\n",
    "    sentence = re.sub(\"ؤ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ئ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ة\", \"ه\", sentence)\n",
    "    sentence = re.sub(\"گ\", \"ك\", sentence)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_normalize_arabic(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings but replace some of chars  like ة to ه Arabic words\n",
    "    '''\n",
    "    text_list = [one_string_normalize_arabic(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_tokenization(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words\n",
    "    '''\n",
    "    sentence = word_tokenize(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_tokenization(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of Strings\n",
    "    return:\n",
    "        list of strings and every string is list of words\n",
    "    '''\n",
    "    text_list = [one_string_tokenization(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_un_tokenization(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of words\n",
    "    return:\n",
    "        string of words\n",
    "    '''\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_un_tokenization(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of words\n",
    "    return:\n",
    "        string of words\n",
    "    '''\n",
    "    text_list = [one_string_un_tokenization(sentence) for sentence in text_list]\n",
    "    return text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_spelling_correction(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of correct words\n",
    "    '''\n",
    "    \n",
    "    sentence = str(TextBlob(sentence).correct())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_spelling_correction(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings each of them are some of words\n",
    "    return:\n",
    "        list of correct strings\n",
    "    '''\n",
    "    text_list = [one_string_spelling_correction(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_steming(sentence, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words with steming which the root of the word\n",
    "    '''\n",
    "    sentence = one_string_tokenization(sentence)\n",
    "    if language == 'English':\n",
    "        stemmer = PorterStemmer()\n",
    "        sentence = [stemmer.stem(word) for word in sentence]\n",
    "    elif language == 'Arabic':\n",
    "        stemmer = ISRIStemmer()\n",
    "        sentence = [stemmer.stem(word) for word in sentence]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_steming(text_list, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings with steming which the root of the word in each string\n",
    "    '''\n",
    "    text_list = [one_string_steming(sentence, language) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_Lemmatizing(sentence, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words with Lemmatizing\n",
    "    '''\n",
    "    sentence = one_string_tokenization(sentence)\n",
    "    if language == 'English':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    elif language == 'Arabic':\n",
    "        stemmer = ArabicLightStemmer()\n",
    "        sentence = [stemmer.light_stem(word) for word in sentence]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_Lemmatizing(text_list, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings with steming which the root of the word in each string\n",
    "    '''\n",
    "    text_list = [one_string_Lemmatizing(sentence, language) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_stop_words(sentence, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        remove stop words from this string like this, did\n",
    "        but other words like not, no dont remove\n",
    "    '''\n",
    "    if language == 'English' or language == 'english':\n",
    "        stop_words = NLP().stopword_list # retrive stopwords list\n",
    "        sentence = sentence.split(' ')\n",
    "        updated_sentence = ''\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                updated_sentence += word + ' '\n",
    "    \n",
    "    elif language == 'Arabic' or language == 'arabic':\n",
    "        stop_words = arabic_stop_words_designed \n",
    "        sentence = sentence.split(' ')\n",
    "        updated_sentence = ''\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                updated_sentence += word + ' '\n",
    "    return updated_sentence\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_stop_words(text_list, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of string\n",
    "    return:\n",
    "        list of string without stop words\n",
    "    '''\n",
    "#     if language == 'English' or language == 'english':\n",
    "#         stop_words = NLP().stopword_list()\n",
    "        \n",
    "    text_list = [one_string_stop_words(sentence, language) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_list_all_words(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of lists each of them are words\n",
    "    return:\n",
    "        one list contain all words\n",
    "    '''\n",
    "    updated_list = []\n",
    "    [updated_list.extend(li) for li in text_list]\n",
    "    return updated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_pip_line(text_list):\n",
    "    text_list = all_string_remove_diacritics(text_list)\n",
    "    text_list = all_strings_remove_punctuation(text_list)\n",
    "    text_list = all_string_normalize_arabic(text_list)\n",
    "    text_list = all_string_Lemmatizing(text_list, 'Arabic')\n",
    "#     text_list = all_string_un_tokenization(text_list)\n",
    "#     text_list = all_string_stop_words(text_list, 'Arabic')\n",
    "    return text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\") \n",
    "payload = {\"text\": \"هذا مثال بسيط\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'هذا مثال بسيط'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { \"content-type\": \"application/json\", \"cache-control\": \"no-cache\", }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content-type': 'application/json', 'cache-control': 'no-cache'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "CannotSendRequest",
     "evalue": "Request-sent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCannotSendRequest\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-abce6efd54c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"farasa-api.qcri.org/msa/webapi/spellcheck\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# res = conn.getresponse()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# data = res.read()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(data.decode(\"utf-8\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1242\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1243\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'skip_accept_encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;31m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mputrequest\u001b[0;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_REQ_STARTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;31m# Save the method we use, we need it later in the response phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCannotSendRequest\u001b[0m: Request-sent"
     ]
    }
   ],
   "source": [
    "conn.request(\"POST\",  \"/msa/webapi/spellcheck\", payload, headers)\n",
    "# res = conn.getresponse() \n",
    "# data = res.read() \n",
    "# print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "“{\\”text\\”: هذا مثال بسيط}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
