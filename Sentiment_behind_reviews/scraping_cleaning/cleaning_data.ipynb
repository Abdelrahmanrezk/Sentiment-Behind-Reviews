{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning scraped reviews\n",
    "\n",
    "**Welcome to second stage of cleaning scraped reviews, what in this notebook:**\n",
    "- Get all reviews from mongo database\n",
    "- pipline process\n",
    "    - one for Arabic reviews\n",
    "    \n",
    "**Pipline process Structure**\n",
    "- Convert all reviews to lower case # for English Reviews\n",
    "- remove punctuations of all reviews\n",
    "- remove stop words # I found its so bad to remove stop words because its convert the meaning of the sentense at all but I have Designed my own stopwords for this project because of sentment analysis issues\n",
    "- spell correction\n",
    "- Tokenization\n",
    "- Steaming\n",
    "- Lemmatization\n",
    "- Frequency of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from souq_configs import *\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import os\n",
    "import pymongo\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import logging\n",
    "import logging.config\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, webtext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import Word, TextBlob\n",
    "from autocorrect import spell\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import unidecode\n",
    "import string\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from nlppreprocess import NLP\n",
    "from tashaphyne.stemming import ArabicLightStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to mongo cloud database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(f\"mongodb+srv://{mongo_user}:{mongo_pass}@{mongo_url}\")\n",
    "db = client.SBR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all prodcuts in our cloud database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = list(db.products.find({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our scraped now are 5777 Product\n"
     ]
    }
   ],
   "source": [
    "print(\"Our scraped now are \" + str(len(products)) + \" Product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one product Data\n",
      "\n",
      " {'_id': ObjectId('5e48539b980cc3c34837af7f'), 'product_title': 'هاتف ابل ايفون 11 مع فيس تايم بشريحة واحدة وشريحة الكترونية - ذاكرة تخزين 64 جيجا، ذاكرة وصول عشوائية 4 جيجا، شبكة ال تي اي من الجيل الرابع - ارجواني', 'product_url': 'https://egypt.souq.com/eg-ar/%D9%87%D8%A7%D8%AA%D9%81-%D8%A7%D8%A8%D9%84-%D8%A7%D9%8A%D9%81%D9%88%D9%86-11-%D9%85%D8%B9-%D9%81%D9%8A%D8%B3-%D8%AA%D8%A7%D9%8A%D9%85-%D8%A8%D8%B4%D8%B1%D9%8A%D8%AD%D8%A9-%D9%88%D8%A7%D8%AD%D8%AF%D8%A9-%D9%88%D8%B4%D8%B1%D9%8A%D8%AD%D8%A9-%D8%A7%D9%84%D9%83%D8%AA%D8%B1%D9%88%D9%86%D9%8A%D8%A9-%D8%B0%D8%A7%D9%83%D8%B1%D8%A9-%D8%AA%D8%AE%D8%B2%D9%8A%D9%86-64-%D8%AC%D9%8A%D8%AC%D8%A7-%D8%B0%D8%A7%D9%83%D8%B1%D8%A9-%D9%88%D8%B5%D9%88%D9%84-%D8%B9%D8%B4%D9%88%D8%A7%D8%A6%D9%8A%D8%A9-4-%D8%AC%D9%8A%D8%AC%D8%A7-%D8%B4%D8%A8%D9%83%D8%A9-%D8%A7%D9%84-%D8%AA%D9%8A-%D8%A7%D9%8A-%D9%85%D9%86-%D8%A7%D9%84%D8%AC%D9%8A%D9%84-%D8%A7%D9%84%D8%B1%D8%A7%D8%A8%D8%B9-%D8%A7%D8%B1%D8%AC%D9%88%D8%A7%D9%86%D9%8A-68312948/i/', 'image_src': 'https://cf1.s3.souqcdn.com/item/2019/09/12/68/31/29/48/item_L_68312948_63bd51c18418a.jpg', 'product_new_price': 14299.0, 'product_old_price': 0.0, 'product_discount_percentage': 0.0, 'product_discount_value': 0.0, 'product_reviews': ['جهاز رائع', 'ممتاز', 'جهاز ما يحتاج تقييم', 'توصيل سريع وسعر ممتاز', 'السلام عليكم ورحمة الله وبركاته لقد تم إيصال الجوال الايفون ولم تصلني الشريحة الإلكترونية أرجو الإفادة', 'ممتاز والتوصيل مره سريع', 'كل شئ يجنن تغليف ممتاز توصيل سريع سعر رائع شكرا سوق', 'شكرا السعر وربي ببلاش كمان وصل سريع', 'المنتج أصلي وسعره منافس وأقل من التطبيقات الأخرى المنتج أصلي وسعره منافس وأقل من التطبيقات الأخرى المنتج أصلي وسعره منافس وأقل من التطبيقات الأخرى', 'لا يوجد', 'Loved', 'يعطيهم العافيه الجهاز وصل سليم وممتاز', 'ممتاز جدا', 'مطابق للمواصفات و سرعه فى التوصيل', 'مثل ما هو مكتوب والتوصيل في الموعد', 'جميييييل', 'ممتاز والسعر كويس', 'رائع', 'صراحة كنت خايفة يطلع مو اصلي لكن الحمدلله طلع اصلي١٠٠٪\\u061c والسعر منافس جدا', 'جميل', 'ممتاز', 'هاتف جميل وأنيق', 'صراحه جوده فاشله جدا', 'منتج اصلي وتوصيل رائع', 'جميل جدا انصح به', 'ممتاز', 'ممتاز جدا وسرعه بخدمة الشحن', 'شكرا لسوق ع السعر الرايع', 'المنتج اصلي ولا يوجد اي ملاحظات وتم توصيله بسرعة شكراً', 'ممتاز', 'الجهاز معروف والتوصيل في الموعد شكراً لكم', 'السعر كان جداً جميل ، التوصيل سريع', 'perfect and delivered within 1 day', 'Good', 'Just changed from Samsung s9 to this , so good to be an iPhone user, build quality is excellent', 'Original .. Thanks', 'السعر رائع والتوصيل سريع جدا', 'At first I was so worried and suspicious, but once it arrived I checked the serial number and it’s as original as it can be, very good, packaging was well done, and the device is amazing', 'I love it and delivery is too fast', 'وصل تاني يوم من الطلب شكرا سوق'], 'main_feature_of_product': ['2019-09-13', '16.4 centimeters', '512 grams', '190199222397', 'ابل', 'iOS', '64 جيجابايت', 'Dual SIM (nano-SIM and eSIM)', 'هواتف ذكية', '4جي ال تي اي', 'ابل', 'MWLX2AH/A', '2000 - 3000 مللي أمبير_ساعة', 'UPC-A', '190199222397', '2724968977898', '12MP Ultra Wide + 12 MP Wide', '6.1 انش', 'ارجواني', '4 جيجابايت'], 'Uploaded_product': False}\n"
     ]
    }
   ],
   "source": [
    "print(\"one product Data\\n\\n\", products[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Reviews\n",
    "Our data now have all of products info not just reviews, so we need to get these reviews beside of this we need to separate Arabic and English reviews.\n",
    "\n",
    "returned products are list each of them is document(objects as key value)\n",
    "\n",
    "### Function structure\n",
    "- First loop over all products\n",
    "- for each product get all reviews\n",
    "- for each review check its language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all_reviews(products):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of products each of them as object with key and value\n",
    "    return:\n",
    "        from this products we just need the reviews so\n",
    "        Arabic Reviews\n",
    "        English Reviews\n",
    "    '''\n",
    "    all_arabic_reviews = []\n",
    "    all_english_reviews = []\n",
    "# Loop over products\n",
    "    for indx,pro_val in enumerate(products):\n",
    "# Loop over reviews\n",
    "        for review in pro_val['product_reviews']:\n",
    "            try:\n",
    "                char_check = review[0]\n",
    "                if char_check >= 'a' and char_check <= 'z' or char_check >= 'A' and char_check <= 'Z':\n",
    "                    all_english_reviews.append(review)\n",
    "                else:\n",
    "                    all_arabic_reviews.append(review)\n",
    "            except Exception as e:\n",
    "# send exception to log folder\n",
    "                logf = open(\"../logs_files/cleaning_reviews_error.log\", \"a+\")\n",
    "                logf.write(\"This error related to function export_all_reviews of cleaning_data file\\n\" \n",
    "                   + str(e) + \"\\n\" + \"#\" *99 + \"\\n\") # \"#\" *99 as separated lines\n",
    "    return all_arabic_reviews, all_english_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare my own stopwords list\n",
    "I have designed these stopwords from multiple resource based on this problem of sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_of_stop_words_to_list(file_dir):\n",
    "    '''\n",
    "    Argument:\n",
    "        file with stop words\n",
    "    return:\n",
    "        list of these stop words\n",
    "    '''\n",
    "    stop_words_designed = []\n",
    "    with open(file_dir, 'r') as file:\n",
    "        file = file.readlines()\n",
    "        file = \"\".join(file)\n",
    "        file = re.sub('[\\[\\]\\'\\\",]', '', file)\n",
    "        stop_words_designed = file.split()\n",
    "    return stop_words_designed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First File\n",
      " 156\n",
      "\n",
      "After Expanding two file together\n",
      " 889\n",
      "\n",
      "Remove Dublicated words\n",
      " 661\n",
      "\n",
      "Update the end file manually to remove some of words\n",
      " 525\n"
     ]
    }
   ],
   "source": [
    "# file_dir = !pwd\n",
    "# file_dir = file_dir[0]\n",
    "# print(file_dir)\n",
    "file_dir1 =  '../stop_words/nltk_stop_words_handle.txt'\n",
    "file_di2 = '../stop_words/stop_list1.txt'\n",
    "file_di3 ='../stop_words/updated_stop_words.txt'\n",
    "stop_words_designed = convert_file_of_stop_words_to_list(file_dir1)\n",
    "print(\"First File\\n\",len(stop_words_designed))\n",
    "stop_words_designed.extend(convert_file_of_stop_words_to_list(file_di2))\n",
    "print(\"\\nAfter Expanding two file together\\n\",len(stop_words_designed))\n",
    "# now remove dublicated via set\n",
    "stop_words_designed = set(stop_words_designed)\n",
    "stop_words_designed = list(stop_words_designed)\n",
    "print(\"\\nRemove Dublicated words\\n\",len(stop_words_designed))\n",
    "arabic_stop_words_designed = convert_file_of_stop_words_to_list(file_di3)\n",
    "print(\"\\nUpdate the end file manually to remove some of words\\n\",len(arabic_stop_words_designed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arabic_reviews, all_english_reviews = export_all_reviews(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Arabic reviews\n",
      " 38695\n",
      "\n",
      "Total English reviews\n",
      " 34166\n",
      "\n",
      "Some of  Arabic reviews\n",
      " ['جهاز ممتاز و تسليم من سوق فوق الممتاز', 'ممتاز شكراً لسوق كوم وللبائع وتوصيل سريع أيضا', 'شيء خرافي', 'ابداع جديد من سامسونج', 'لايوجد']\n",
      "\n",
      "some of English reviews\n",
      " ['I love it', 'soug really doing good service to society | compare to the local market the prices are very less - minimum 5%', 'Faster delivery ,genuine product. ..Cheaper in price as compare to any other online shopping. Must try this seller...Thanks Souq..', 'Genuine Product', 'Nothing']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Arabic reviews\\n\", len(all_arabic_reviews))\n",
    "print(\"\\nTotal English reviews\\n\", len(all_english_reviews))\n",
    "print(\"\\nSome of  Arabic reviews\\n\", all_arabic_reviews[:5])\n",
    "print(\"\\nsome of English reviews\\n\", all_english_reviews[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove dublicate entry first\n",
    "use set features of python which return all of unique reviews\n",
    "then return as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Total Arabic reviews\n",
      " 20494\n",
      "\n",
      "Now Total English reviews\n",
      " 18950\n"
     ]
    }
   ],
   "source": [
    "all_arabic_reviews = set(all_arabic_reviews)\n",
    "all_english_reviews = set(all_english_reviews)\n",
    "all_arabic_reviews = list(all_arabic_reviews)\n",
    "all_english_reviews = list(all_english_reviews)\n",
    "print(\"Now Total Arabic reviews\\n\", len(all_arabic_reviews))\n",
    "print(\"\\nNow Total English reviews\\n\", len(all_english_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep only reviews more than one words because maybe work like قليلة has no meaning but  جودة قليلة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_those_reviews_greater_1_word(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of reviews\n",
    "    return:\n",
    "        list of reviews each of them grater than or equal 2 words\n",
    "    '''\n",
    "    updated_reviews = []\n",
    "    for review in text_list:\n",
    "        if len(review.split()) >= 2:\n",
    "            updated_reviews.append(review)\n",
    "            \n",
    "    return updated_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Total Arabic reviews\n",
      " 19870\n",
      "\n",
      "Now Total English reviews\n",
      " 18220\n"
     ]
    }
   ],
   "source": [
    "all_arabic_reviews = keep_those_reviews_greater_1_word(all_arabic_reviews)\n",
    "all_english_reviews = keep_those_reviews_greater_1_word(all_english_reviews)\n",
    "print(\"Now Total Arabic reviews\\n\", len(all_arabic_reviews))\n",
    "print(\"\\nNow Total English reviews\\n\", len(all_english_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_of_strings_to_list_of_words(text):\n",
    "    '''\n",
    "    A function used to convert list of strings to be list of all words of these strings like:\n",
    "    ['تليفون جيد',\n",
    " 'ايباد ميني فور من افضل الايبادات في السوق وسعره كان مغري',]\n",
    "    return:\n",
    "        list of all words in all string like:\n",
    "        ['تليفون', 'جيد'] and so on\n",
    "    '''\n",
    "    word_list = []\n",
    "    for i in text:\n",
    "        i = i.split()\n",
    "        word_list.extend(i)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_lower_conversation(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        text as string of words\n",
    "    return:\n",
    "        lower of this string\n",
    "    '''\n",
    "    return sentence.lower()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_lower_conversation(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings and each of these strings does contain some of words\n",
    "    return:\n",
    "        lower each string in this list\n",
    "    '''\n",
    "    text_list = [one_string_lower_conversation(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove some of special characters characters**\n",
    "\n",
    "like: َ\"  ُ  ْ  \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_remove_diacritics(sentence):\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    sentence = re.sub(noise, '', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_remove_diacritics(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of string without special chars from Arabic language\n",
    "    '''\n",
    "    text_list = [one_string_remove_diacritics(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove some of puncatution and repeated words**\n",
    "\n",
    "like: $%&'()*+,-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_remove_punctuation(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    reutrn:\n",
    "        string without punctuation like [.!?] and others\n",
    "    '''\n",
    "    sentence = sentence.split(' ')\n",
    "    strs = ''\n",
    "    punctuations = string.punctuation\n",
    "    for word in sentence:\n",
    "#         word = re.sub(r'(.)\\1+', r'\\1', word) # remove repated chars\n",
    "        word = re.sub('[^\\w\\s+]',' ',word)\n",
    "        if len(word) > 1 and not (word[0] >= 'a' and word[0] < 'z' or word[0] >= 'A' and word[0] < 'Z'):\n",
    "            strs += word + ' '\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    strs.translate(translator)\n",
    "    return strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_strings_remove_punctuation(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings \n",
    "    reutrn:\n",
    "        list of strings without punctuation like [.!?] and others\n",
    "    '''\n",
    "    text_list = [one_string_remove_punctuation(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_normalize_arabic(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of words but standardize the words\n",
    "    '''\n",
    "    sentence = re.sub(\"[إأآا]\", \"ا\", sentence)\n",
    "    sentence = re.sub(\"ى\", \"ي\", sentence)\n",
    "    sentence = re.sub(\"ؤ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ئ\", \"ء\", sentence)\n",
    "    sentence = re.sub(\"ة\", \"ه\", sentence)\n",
    "    sentence = re.sub(\"گ\", \"ك\", sentence)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_normalize_arabic(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings but replace some of chars  like ة to ه Arabic words\n",
    "    '''\n",
    "    text_list = [one_string_normalize_arabic(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_tokenization(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words\n",
    "    '''\n",
    "    sentence = word_tokenize(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_tokenization(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of Strings\n",
    "    return:\n",
    "        list of strings and every string is list of words\n",
    "    '''\n",
    "    text_list = [one_string_tokenization(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_un_tokenization(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of words\n",
    "    return:\n",
    "        string of words\n",
    "    '''\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_un_tokenization(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of words\n",
    "    return:\n",
    "        string of words\n",
    "    '''\n",
    "    text_list = [one_string_un_tokenization(sentence) for sentence in text_list]\n",
    "    return text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_spelling_correction(sentence):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        string of correct words\n",
    "    '''\n",
    "    \n",
    "    sentence = str(TextBlob(sentence).correct())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_spelling_correction(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings each of them are some of words\n",
    "    return:\n",
    "        list of correct strings\n",
    "    '''\n",
    "    text_list = [one_string_spelling_correction(sentence) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_steming(sentence, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words with steming which the root of the word\n",
    "    '''\n",
    "    sentence = one_string_tokenization(sentence)\n",
    "    if language == 'English':\n",
    "        stemmer = PorterStemmer()\n",
    "        sentence = [stemmer.stem(word) for word in sentence]\n",
    "    elif language == 'Arabic':\n",
    "        stemmer = ISRIStemmer()\n",
    "        sentence = [stemmer.stem(word) for word in sentence]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_steming(text_list, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings with steming which the root of the word in each string\n",
    "    '''\n",
    "    text_list = [one_string_steming(sentence, language) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_Lemmatizing(sentence, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        String of words\n",
    "    return:\n",
    "        list of words with Lemmatizing\n",
    "    '''\n",
    "    sentence = one_string_tokenization(sentence)\n",
    "    if language == 'English':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    elif language == 'Arabic':\n",
    "        stemmer = ArabicLightStemmer()\n",
    "        sentence = [stemmer.light_stem(word) for word in sentence]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_Lemmatizing(text_list, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of strings\n",
    "    return:\n",
    "        list of strings with steming which the root of the word in each string\n",
    "    '''\n",
    "    text_list = [one_string_Lemmatizing(sentence, language) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_string_stop_words(sentence, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        string of words\n",
    "    return:\n",
    "        remove stop words from this string like this, did\n",
    "        but other words like not, no dont remove\n",
    "    '''\n",
    "    if language == 'English' or language == 'english':\n",
    "        stop_words = NLP().stopword_list # retrive stopwords list\n",
    "        sentence = sentence.split(' ')\n",
    "        updated_sentence = ''\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                updated_sentence += word + ' '\n",
    "    \n",
    "    elif language == 'Arabic' or language == 'arabic':\n",
    "        stop_words = arabic_stop_words_designed \n",
    "        sentence = sentence.split(' ')\n",
    "        updated_sentence = ''\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                updated_sentence += word + ' '\n",
    "    return updated_sentence\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_string_stop_words(text_list, language):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of string\n",
    "    return:\n",
    "        list of string without stop words\n",
    "    '''\n",
    "#     if language == 'English' or language == 'english':\n",
    "#         stop_words = NLP().stopword_list()\n",
    "        \n",
    "    text_list = [one_string_stop_words(sentence, language) for sentence in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_list_all_words(text_list):\n",
    "    '''\n",
    "    Argument:\n",
    "        list of lists each of them are words\n",
    "    return:\n",
    "        one list contain all words\n",
    "    '''\n",
    "    updated_list = []\n",
    "    [updated_list.extend(li) for li in text_list]\n",
    "    return updated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_pip_line(text_list):\n",
    "    text_list = all_string_remove_diacritics(text_list)\n",
    "    text_list = all_strings_remove_punctuation(text_list)\n",
    "    text_list = all_string_normalize_arabic(text_list)\n",
    "#     text_list = all_string_Lemmatizing(text_list, 'Arabic')\n",
    "#     text_list = all_string_un_tokenization(text_list)\n",
    "    text_list = all_string_stop_words(text_list, 'Arabic')\n",
    "    return text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
